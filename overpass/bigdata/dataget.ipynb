{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import HiveContext,SparkSession\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "\n",
    "appname = \"Test Application\"\n",
    "master = \"spark://master:7077\"\n",
    "conf = SparkConf().setAppName(appname).setMaster(master).set('spark.driver.maxResultSize', '8g') # maxResultSize work提交给dravel节点的最大数据\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate()\n",
    "hive_cont = HiveContext(spark)\n",
    "spark.sql(\"set spark.sql.execution.arrow.enabled=true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL获取HIVE数据库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 需从头遍历一次数据，以获取 overpass_abute 积水点属性表\n",
    "### 广播变量不支持遍历修改，累加器无法生成对应的表格"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 从所有数据中 去重生成 S_NO 列表 (为方便，将数据写入 Hive rcxljjs.t_jishui_sno 表中)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从所有数据中 去重生成 S_NO 列表 (为方便，将数据写入 Hive rcxljjs.t_jishui_sno 表中)\n",
    "start_time = '2013-01-01 00:00:00'\n",
    "end_time = '2022-01-01 00:00:00'\n",
    "\n",
    "overpass_sql = open(file='./sql/overpass.sql')\n",
    "list_text = overpass_sql.readlines()\n",
    "overpass_sql.close()\n",
    "sql_text = \" \".join(list_text)\n",
    "sql_text = sql_text.format(start_time=start_time, end_time=end_time)\n",
    "# Hive 拿数据\n",
    "hive_data = spark.sql(sql_text)\n",
    "\n",
    "# 唯一的S_NO列表，表示所有的下立交积水点ID\n",
    "list_no = hive_data.select('S_NO').dropDuplicates(['S_NO'])\n",
    "list_no.write.format(\"hive\").mode(\"overwrite\").saveAsTable('rcxljjs.t_jishui_sno')\n",
    "\n",
    "# 唯一的S_NO属性表\n",
    "overpass_abute = hive_data.select('S_NO','T_SYSTIME', 'S_HASMONITOR', 'S_STATENAME', 'S_ADDR','S_BUILDDATE',\n",
    "                                  'S_PROUNIT', 'S_MANAGE_UNIT','S_MAINTAIN_UNIT', 'S_STATIONID', 'S_STATIONNAME')\n",
    "overpass_abute = overpass_abute.orderBy(['S_NO','T_SYSTIME'],ascending=[0,0]).dropDuplicates(['S_NO'])\n",
    "\n",
    "# overpass_abute.write.format(\"hive\").mode(\"overwrite\").saveAsTable('rcxljjs.t_overpass_abute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overpass_abute = overpass_abute.cache().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 按时间、S_NO遍历积水监测数据，生成 hydrops_data 数据表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一步：积水记录表   hydrops_data\n",
    "# hydrops_data 记录所有积水的开始结束时间，深度与等级\n",
    "# 从hive 多张表中获取所需数据 -- 已按时间进行排序的数据\n",
    "years = {'2013':'2014',\n",
    "         '2014':'2015',\n",
    "         '2015':'2016',\n",
    "         '2016':'2017',\n",
    "         '2017':'2018',\n",
    "         '2018':'2019',\n",
    "         '2019':'2020',\n",
    "         '2020':'2021',\n",
    "         '2021':'2022'}\n",
    "\n",
    "# 可用的S_NO名单\n",
    "list_no = spark.sql('select * from rcxljjs.t_jishui_sno').cache().toPandas()\n",
    "\n",
    "# 初始化DataFrame，hydrops_data 作为最终的积水记录，可存入Hive\n",
    "hydrops_data = pd.DataFrame(columns=['S_NO', 'START_TIME', 'END_TIME', 'DEEP', 'JSRANK'])\n",
    "hydrops_data_index = 0\n",
    "\n",
    "log= 0  # 标志位，1代表正在积水\n",
    "start_time = '2010-12-22 00:00:00'\n",
    "end_time = '2010-12-22 00:00:00'\n",
    "water_deep = 0.0\n",
    "jsrank = 0\n",
    "\n",
    "\n",
    "# 定于分布式 将Spark df 转换为 Pandas df\n",
    "def _map_to_pandas(rdds):\n",
    "    return [pd.DataFrame(list(rdds))]\n",
    "    \n",
    "def toPandas(df, n_partitions=None):\n",
    "    if n_partitions is not None: df = df.repartition(n_partitions)\n",
    "    df_pand = df.rdd.mapPartitions(_map_to_pandas).collect()\n",
    "    df_pand = pd.concat(df_pand)\n",
    "    df_pand.columns = df.columns\n",
    "    return df_pand\n",
    "\n",
    "for start_year,end_year in years.items():\n",
    "    # 按时间范围截取数据，to_pandas不宜返回太大数据\n",
    "    start_time = start_year + '-01-01 00:00:00'\n",
    "    end_time = end_year + '-01-01 00:00:00'\n",
    "    \n",
    "    overpass_sql = open(file='./sql/overpass.sql')\n",
    "    list_text = overpass_sql.readlines()\n",
    "    overpass_sql.close()\n",
    "    sql_text = \" \".join(list_text)\n",
    "    sql_text = sql_text.format(start_time=start_time, end_time=end_time)\n",
    "    # Hive 拿数据\n",
    "    hive_data = spark.sql(sql_text)\n",
    "    \n",
    "    hive_data.cache()\n",
    "    \n",
    "#     # 采用自定义优化的方式转换 Pandadf_panddf\n",
    "#     %timeit toPandas(overpass_data)\n",
    "\n",
    "    # 采用 Arrow 的方式优化转换\n",
    "    # spark.sql(\"set spark.sql.execution.arrow.enabled=true\")\n",
    "    overpass_data = hive_data.toPandas()\n",
    "    \n",
    "    # 根据 S_NO 筛选特定ID的积水点\n",
    "    for s_no in list_no.itertuples(index=True):\n",
    "        s_no = s_no.S_NO\n",
    "        JSD_value = overpass_data[overpass_data['S_NO'] == s_no][['S_NO', 'T_SYSTIME', 'N_VALUE']]\n",
    "        JSD_value['T_SYSTIME'] = pd.to_datetime(JSD_value['T_SYSTIME'])\n",
    "        JSD_value.sort_values('T_SYSTIME', inplace=True)\n",
    "        \n",
    "        # 对每一行数据进行遍历，判断其积水状态并记录\n",
    "        for value in JSD_value.itertuples(index=True):\n",
    "            if value.N_VALUE >= 10 and log == 1:  # 正在积水并还在积水\n",
    "                water_deep = max(water_deep, value.N_VALUE)\n",
    "            if value.N_VALUE < 10 and log == 1:  # 正在积水但在此时退出积水\n",
    "                if start_time == '2010-12-22 00:00:00': start_time = value.T_SYSTIME\n",
    "                end_time = value.T_SYSTIME\n",
    "                if 10 <= water_deep < 15:  # 判断积水深度等级\n",
    "                    jsrank = 1\n",
    "                elif 15 <= water_deep < 25:\n",
    "                    jsrank = 2\n",
    "                elif 25 <= water_deep < 50:\n",
    "                    jsrank = 3\n",
    "                elif water_deep >= 50:\n",
    "                    jsrank = 4\n",
    "                hydrops_data.loc[hydrops_data_index] = [value.S_NO, start_time, end_time, water_deep, jsrank]\n",
    "                log, water_deep = 0, 0.0\n",
    "                hydrops_data_index += 1\n",
    "            if value.N_VALUE >= 10 and log == 0:  # 不积水，此时开始积水\n",
    "                start_time = value.T_SYSTIME\n",
    "                log = 1\n",
    "                water_deep = max(water_deep, value.N_VALUE)\n",
    "            if value.N_VALUE < 10 and log == 0: continue  # 不积水\n",
    "    \n",
    "# 将 hydrops_data 保存至Hive数据库 rcxljjs.t_hydrops_data\n",
    "\n",
    "\n",
    "hydrops_data_value = hydrops_data.values.tolist()\n",
    "# Pandas df 中的时间格式为Timestamp，需转化为spark中的 datatime\n",
    "for i in hydrops_data_value:\n",
    "    i[1] = i[1].to_pydatetime()\n",
    "    i[2] = i[2].to_pydatetime()\n",
    "hydrops_data_columns = list(hydrops_data.columns)\n",
    "\n",
    "hydrops_spark = spark.createDataFrame(hydrops_data_value, hydrops_data_columns)\n",
    "hydrops_spark.write.format(\"hive\").mode(\"overwrite\").saveAsTable('rcxljjs.t_hydrops_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建属性表 overpass_abute，记录最大深度，平均深度与积水次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从所有数据中 去重生成 S_NO 列表 (为方便，将数据写入 Hive rcxljjs.t_jishui_sno 表中)\n",
    "\n",
    "overpass_abute = spark.sql('select * from rcxljjs.t_overpass_abute').cache().toPandas()\n",
    "\n",
    "\n",
    "hydrops_deep = hydrops_data[['S_NO', 'DEEP']]\n",
    "# 统计 积水平均深度\n",
    "deep_mean = hydrops_deep.groupby('S_NO').mean()\n",
    "# 统计 积水最大深度\n",
    "deep_max = hydrops_deep.groupby('S_NO').max()\n",
    "\n",
    "# 索引 积水点的S_NO\n",
    "deep_SNO = hydrops_deep['S_NO']\n",
    "deep_SNO_list = list(deep_SNO)\n",
    "overpass_abute[['MEAN_DEEP', 'MAX_DEEP', 'FREQU']] = ''\n",
    "for index in deep_SNO:\n",
    "    # 记录平均深度\n",
    "    overpass_abute.loc[index, 'MEAN_DEEP'] = deep_mean.loc[index, 'DEEP']\n",
    "    overpass_abute.loc[index, 'MAX_DEEP'] = deep_max.loc[index, 'DEEP']\n",
    "    overpass_abute.loc[index, 'FREQU'] = deep_SNO_list.count(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
